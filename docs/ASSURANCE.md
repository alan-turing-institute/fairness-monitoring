# Assurance of Fairness 

The quest for "fair" data and algorithms in AI development is crucial for ensuring overall AI safety. In 2023, the UK government organized an international summit to address AI risks and discuss coordinated mitigation strategies. One significant outcome of this summit is the [Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023), which emphasizes understanding AI risks within _"the context of a wider global approach to understanding the impact of AI in our societies."_

In line with both national and international initiatives, the UK's Responsible Technology Adoption Unit regularly releases AI assurance use cases. Two examples from the financial services sector are as follows:

- **[Case 1 - Clear Bank](https://www.gov.uk/ai-assurance-techniques/clearbank-safeguarding-generative-ai-use-cases-in-a-regulated-fintech-banking-api):** Clear Bank focuses on integrating LLM (Limited License Model). They ensure fairness by consistently involving human oversight, including domain experts for continuous review and implementing feedback mechanisms to detect biases at every pipeline stage. ([Link to platform page](https://clear.bank/))

- **[Case 2 - Credo Reinsurance](https://www.gov.uk/ai-assurance-techniques/credo-ai-governance-platform-reinsurance-provider-algorithmic-bias-assessment-and-reporting):** Credo Reinsurance's case centers on their machine learning operations (MLOps) system. Their Credo AI system automatically identifies biases in models pre-deployment. The platform evaluates fairness outcomes against established standards. However, due to compliance with anti-discrimination regulations (e.g., NYCâ€™s LL-144), they can only compile necessary data quarterly, not in real-time. ([Link to platform page](https://www.credo.ai/product))


**TODO:**
- [ ] Add SME experiences whilst using our tool

